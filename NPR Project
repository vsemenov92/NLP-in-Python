import pandas as pd
npr = pd.read_csv('npr.csv')

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_df=0.9,min_df=2,stop_words='english')
dtm = cv.fit_transform(npr['Article'])
dtm

from sklearn.decomposition import LatentDirichletAllocation

# n_components is number of topics and random_state is seed
LDA = LatentDirichletAllocation(n_components=7,random_state=42)
LDA.fit(dtm)

import random 
random_word_id = random.randint(0,54777)
cv.get_feature_names()[random_word_id]

type(LDA.components_)
LDA.components_
single_topic = LDA.components_[0]
single_topic.argsort()
import numpy as np
arr = np.array([10,200,1])
arr.argsort()

# INDEX POSITIONS SORTED FROM LEAST TO GREATEST
single_topic.argsort()[-10:]

top_ten_words = single_topic.argsort()[-10:]
top_twenty_words = single_topic.argsort()[-20:]

# Grab the highest probabilities of words per topic
for index,topic in enumerate(LDA.components_):
    print(f"THE TOP 15 WORDS FOR TOPIC #{index}")
    print([cv.get_feature_names()[index] for index in topic.argsort()[-15:]])
    print('\n')
    print('\n')
    
topic_results = LDA.transform(dtm)
topic_results[0].argmax()
npr['Topic'] = topic_results.argmax(axis=1)
npr
